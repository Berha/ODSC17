{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# FORTUNE-TELLING WITH PYTHON: AN INTRO TO TIME SERIES MODELING\n",
    "## Jonathan Balaban\n",
    "Connect with me on [LinkedIn](https://www.linkedin.com/in/jbalaban) and [Github](https://github.com/ultimatist)\n",
    "\n",
    "![](https://img.evbuc.com/https%3A%2F%2Fcdn.evbuc.com%2Fimages%2F27728797%2F150577564943%2F1%2Foriginal.jpg?h=230&w=460&rect=0%2C165%2C932%2C466&s=0b175e424f9e6fe378d96daf5714fc50)\n",
    "\n",
    "## Introduction\n",
    "We often refer to our input features in machine learning as \"dimensions\". On that note, there's a dimension that pervades almost everything we do and observe as humans. It's the fourth dimension we experience every waking moment: time. But time is quite unlike other data we capture, and often requires unique machine learning approaches. These models and approaches are fairly established in the R language, but have more recently immigrated to Python.\n",
    "\n",
    "In regession and classification, we use features (collected during a cross-sectional study/survey/measurement) to predict an outcome. The model and parameters represent part of the underlying relationship between features and outcome. But what if we run out of funds to cross-section (it's possible), or need to predict future outcomes for which the features aren't measurable or don't yet exist?\n",
    "\n",
    "### Examples of time series data and modeling (constant time interval):\n",
    "- Hedge fund prediction of stock and index movements\n",
    "- Long and short-term weather forecasting\n",
    "- Business budgeting and trend analysis\n",
    "- Health vitals monitoring\n",
    "- Traffic flows and logistic optimization modeling\n",
    "- Can you think of others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Time series data usually contains more than meets the eye, and can often be decomposed into trend, seasonal, and random fluctuation components.\n",
    "\n",
    "![Decomposition](http://rstatistics.net/wp-content/uploads/2014/09/Multiplicative-Decomposition-of-Time-series.png)\n",
    "\n",
    "- Trends\n",
    "    - Up\n",
    "    - Down\n",
    "    - Flat\n",
    "    - Larger trends can be made up of smaller trends\n",
    "    - There is no defined timeframe for what constitutes a trend; it depends on your goals\n",
    "- Seasonal Effects\n",
    "    - Weekend retail sales spikes\n",
    "    - Holiday shopping\n",
    "    - Energy requirement changes with annual weather patterns\n",
    "    - Note: twitter spikes when news happens are not seasonal; they aren't regular and predictable\n",
    "- Random Fluctuations\n",
    "    - The human element\n",
    "    - Aggregations of small influencers\n",
    "    - Observation errors\n",
    "    - The smaller this is in relation to Trend and Seasonal, the better we can predict the future\n",
    "    \n",
    "Time series models fall into [two camps](http://www.abs.gov.au/websitedbs/D3310114.nsf/home/Time+Series+Analysis:+The+Basics#HOW%20DO%20I%20KNOW%20WHICH%20DECOMPOSITION):\n",
    "- Additive\n",
    "    - Data = Trend + Seasonal + Random\n",
    "    - What we will be using for our modeling\n",
    "- Multiplicative\n",
    "    - Data = Trend x Seasonal x Random\n",
    "    - As easy to fit as Additive if we take the log\n",
    "        - log(Data) = log(Trend x Seasonal x Random)\n",
    "\n",
    "We should use multiplicative models when the percentage change of our data is more important than the absolute value change (e.g. stocks, commodities); as the trend rises and our values grow, we see amplitude growth in seasonal and random fluctuations. If our seasonality and fluctuations are stable, we likely have an additive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Time Series Modeling Process\n",
    "Time series model selection is driven by the Trend and Seasonal components of our raw data. The general approach for analysis looks like this:\n",
    "\n",
    "1. Plot the data and determine Trends and Seasonality\n",
    "    1. Difference or take the log of the data (multiple times if needed) to remove trends for [certain model applications](https://en.wikipedia.org/wiki/Stationary_process)\n",
    "    1. Stationairity is needed for ARMA models\n",
    "1. Determine if we have additive or multiplicative data patterns\n",
    "1. Select the appropriate algorithm based on the chart below\n",
    "1. Determine if model selection is correct with these tools\n",
    "    - Ljung-Box Test\n",
    "    - Residual Errors (Normal Distribution with zero mean and constant variance-homoskedastic, i.i.d)\n",
    "    - Autocorrelation Function (ACF)\n",
    "    - Partial Autocorrelation Function (PACF)\n",
    "\n",
    "Algorithm | Trend | Seasonal | Correlations\n",
    "---|---|---|---\n",
    "ARIMA | X |X|X\n",
    "SMA Smoothing |X||\n",
    "Simple Exponential Smoothing |X||\n",
    "Seasonal Adjustment |X|X|\n",
    "Holt's Exponential Smoothing |X||\n",
    "Holt-Winters |X|X|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## How to achieve and test for stationarity:\n",
    "\n",
    "- The mean of the series is not a function of time:\n",
    "![](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Mean_nonstationary.png)\n",
    "\n",
    "- The variance of the series is not a function of time (homoscedasticity):\n",
    "![](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Var_nonstationary.png)\n",
    "\n",
    "- The covariance at different lags is not a function of time:\n",
    "![](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Cov_nonstationary.png)\n",
    "\n",
    "[From A Complete Tutorial on Time Series Modeling in R](https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/)\n",
    "\n",
    "- [Info on stationarity](http://www.investopedia.com/articles/trading/07/stationary.asp)\n",
    "- Plotting Rolling Statistics\n",
    "    - Plot the moving average/variance and see if it changes with time. This visual technique can be done on different windows, but isn't as rigorously defensible as the test below.\n",
    "- Dickey-Fuller Test\n",
    "    - Statistical tests for checking stationarity; the null hypothesis is that the TS is non-stationary. If our test statistic is below an `alpha` value, we _can_ reject the null hypothesis and say that the series is stationary.\n",
    "\n",
    "$$ Y_t = \\rho * Y_{t-1} + \\epsilon_t \\\\$$\n",
    "$$  Y_t - Y_{t-1} = (\\rho - 1) Y_{t - 1} + \\epsilon_t \\\\$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!pip install pyflux\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyflux as pf\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create a play dataframe from 1-10 (linear and squared) to test how differencing works\n",
    "play = pd.DataFrame([[x for x in range(1,11)], [x**2 for x in range(1,11)]]).T\n",
    "play.columns = ['original', 'squared']\n",
    "play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# stationarize linear series (mean and variance doesn't change for sub-windows)\n",
    "play.original.diff()\n",
    "# this is similar to taking a first-order derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# stationarize squared series\n",
    "play.squared.diff().diff()\n",
    "# notice we need to difference twice on an exponential trend, and every time we do, we lose a bit of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# stationarize squared with log\n",
    "np.log(play.squared)\n",
    "# somewhat works, not as dramatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Prep and EDA\n",
    "\n",
    "We'll be looking at [monthly average temperatures between 1907-1972](https://datamarket.com/data/set/22o4/mean-monthly-temperature-1907-1972#!ds=22o4&display=line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load data, recast columns if needed, convert to datetime\n",
    "monthly_temp = pd.read_csv('./mean-monthly-temperature-1907-19.csv', skipfooter=2, \n",
    "                           infer_datetime_format=True, header=1, index_col=0, names=['month', 'temp'])\n",
    "#monthly_temp.temp = monthly_temp.temp.astype(float)\n",
    "monthly_temp.index = monthly_temp.index.to_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# describe\n",
    "monthly_temp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# resample to annual and plot each\n",
    "annual_temp = monthly_temp.resample('A').mean()\n",
    "monthly_temp.plot();\n",
    "annual_temp.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot both on same figure\n",
    "plt.plot(monthly_temp)\n",
    "plt.plot(annual_temp);\n",
    "\n",
    "# note, easier to see trends on resampled or moving average charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot with plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "data = [go.Scatter(x=annual_temp.index, y=annual_temp.temp)]\n",
    "py.iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot binned yearly segments using resample method\n",
    "monthly_temp.resample('A').temp.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# violinplot months to determine variance and range\n",
    "sns.violinplot(x=monthly_temp.index.month, y=monthly_temp.temp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Are these datasets stationary? We can look at a few things per the list above, including a visual check (there seems to be a small upward trend in the annual, too hard to tell for monthly), a standard deviation check on various differences (smallest one is usually most stationary), and the formal Dickey-Fuller test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check montly deviations for various diffs\n",
    "print(monthly_temp.temp.std())\n",
    "print(monthly_temp.temp.diff().std())\n",
    "print(monthly_temp.temp.diff().diff().std()) # theoretically lowest, but one above is close enough\n",
    "print(monthly_temp.temp.diff().diff().diff().std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# check annual deviations for various diffs\n",
    "print(annual_temp.temp.std()) # looks stationary as is\n",
    "print(annual_temp.temp.diff().std())\n",
    "print(annual_temp.temp.diff().diff().std())\n",
    "print(annual_temp.temp.diff().diff().diff().std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define Dickey-Fuller Test (DFT) function\n",
    "import statsmodels.tsa.stattools as ts\n",
    "def dftest(timeseries):\n",
    "    dftest = ts.adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)\n",
    "    #Determing rolling statistics\n",
    "    rolmean = pd.rolling_mean(timeseries, window=12)\n",
    "    rolstd = pd.rolling_std(timeseries, window=12)\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# run DFT on monthly\n",
    "dftest(monthly_temp.temp)\n",
    "# p-value allows us to reject a unit root: data is stationary\n",
    "\n",
    "# this test is ALL ABOUT mean reversion: This process refers to a time series that displays a tendency to\n",
    "# revert to its historical mean value. Mathematically, such a (continuous) time series is referred to as an Ornstein-Uhlenbeck\n",
    "# process. This is in contrast to a random walk (Brownian motion), which has no \"memory\" of where it has been at each particular \n",
    "# instance of time. The mean-reverting property of a time series can be exploited for better prediction.\n",
    "\n",
    "# discuss brownian motion on a chess board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### SOL\n",
    "\n",
    "One of the key trading concepts in the quantitative toolbox is that of mean reversion. This process refers to a time series that displays a tendency to revert to its historical mean value. Mathematically, such a (continuous) time series is referred to as an Ornstein-Uhlenbeck process. This is in contrast to a random walk (Brownian motion), which has no \"memory\" of where it has been at each particular instance of time. The mean-reverting property of a time series can be exploited in order to produce profitable trading strategies.\n",
    "\n",
    "A continuous mean-reverting time series can be represented by an Ornstein-Uhlenbeck stochastic differential equation:\n",
    "\n",
    "dxt=θ(μ−xt)dt+σdWt\n",
    " \n",
    "Where θ\n",
    " is the rate of reversion to the mean, μ\n",
    " is the mean value of the process, σ\n",
    " is the variance of the process and Wt\n",
    " is a Wiener Process or Brownian Motion.\n",
    "\n",
    "In a discrete setting the equation states that the change of the price series in the next time period is proportional to the difference between the mean price and the current price, with the addition of Gaussian noise.\n",
    "\n",
    "This property motivates the Augmented Dickey-Fuller Test, which we will describe below.\n",
    "\n",
    "https://www.quantstart.com/articles/Basics-of-Statistical-Mean-Reversion-Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# run DFT on annual\n",
    "dftest(annual_temp.temp)\n",
    "# p-value allows us to reject a unit root: data is stationary\n",
    "\n",
    "# here's an example of non-stationary with DFT results\n",
    "# dftest(np.exp(annual_temp.temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ARIMA with Statsmodels\n",
    "Enter [Autoregressive Integrated Moving Average (ARIMA)](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) modeling. When we have autocorrelation between outcomes and their ancestors, we will see a theme, or relationship in the outcome plot. This relationship can be modeled in its own way, allowing us to predict the future with a confidence level commensurate to the strength of the relationship and the proximity to known values (prediction weakens the further out we go).\n",
    "\n",
    "- [ARIMA in R](https://www.otexts.org/fpp/8/5)\n",
    "- [Duke ARIMA Guide](https://people.duke.edu/~rnau/411arim2.htm)\n",
    "- [Great explanation on MA in practice](http://stats.stackexchange.com/questions/164824/moving-average-ma-process-numerical-intuition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![SARIMA Form](https://www.otexts.org/sites/default/files/fpp/images/sarima1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOL\n",
    "\n",
    "Other methods that we won't go into:\n",
    "- Vector autoregressions (VARs)\n",
    "- Gaussian state space models – often called structural time series or unobserved component models\n",
    "- GARCH\n",
    "- Generalized Autoregressive Score (GAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define helper function for visualization\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "def plots(data, lags=None):\n",
    "    layout = (1, 3)\n",
    "    raw  = plt.subplot2grid(layout, (0, 0))\n",
    "    acf  = plt.subplot2grid(layout, (0, 1))\n",
    "    pacf = plt.subplot2grid(layout, (0, 2))\n",
    "    \n",
    "    data.plot(ax=raw)\n",
    "    smt.graphics.plot_acf(data, lags=lags, ax=acf)\n",
    "    smt.graphics.plot_pacf(data, lags=lags, ax=pacf)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plots(monthly_temp, lags=75);\n",
    "# open Duke guide for visual\n",
    "# we note a 12-period cycle (yearly) with suspension bridge design, so must use SARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Box-Jenkins Method](https://en.wikipedia.org/wiki/Box–Jenkins_method)\n",
    "\n",
    "ACF Shape|Indicated Model\n",
    "---|---\n",
    "Exponential, decaying to zero|Autoregressive model. Use the partial autocorrelation plot to identify the order of the autoregressive model.\n",
    "Alternating positive and negative, decaying to zero|Autoregressive model. Use the partial autocorrelation plot to help identify the order.\n",
    "One or more spikes, rest are essentially zero|Moving average model, order identified by where plot becomes zero.\n",
    "Decay, starting after a few lags|Mixed autoregressive and moving average (ARMA) model.\n",
    "All zero or close to zero|Data are essentially random.\n",
    "High values at fixed intervals|Include seasonal autoregressive term.\n",
    "No decay to zero|Series is not stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we might need to install dev version for statespace functionality\n",
    "#!pip install git+https://github.com/statsmodels/statsmodels.git\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# fit SARIMA monthly based on helper plots\n",
    "sar = sm.tsa.statespace.SARIMAX(monthly_temp.temp, order=(3,0,0), seasonal_order=(0,1,1,12), trend='c').fit()\n",
    "sar.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot resids\n",
    "plots(sar.resid, lags=40);\n",
    "\n",
    "# Thought process:\n",
    "# 010010 is overdiff by AIC and negative ACR, but 000010 is a big underdiff with better AIC\n",
    "# we pick 000010,12 and Trend='c' per rule4/5\n",
    "\n",
    "# now look at seasonal, notice negative ACR spike at 12: per rule 13, we add a SMA term\n",
    "# big drop to 4284 AIC\n",
    "# looks like ACR looks good at seasonal lags, so we move back to ARIMA portion\n",
    "\n",
    "# rule6 says we're a bit underdiff, so we add AR=3 based on PACF: 4261 AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot residual diagnostics\n",
    "sar.plot_diagnostics();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot predictions\n",
    "monthly_temp['forecast'] = sar.predict(start = 750, end= 790, dynamic=10)  \n",
    "monthly_temp[730:][['temp', 'forecast']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Tests\n",
    "\n",
    "\n",
    "- [Normality (Jarque-Bera)](http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_normality.html#statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_normality)\n",
    "    - Null hypothesis is normally distributed residuals (good, plays well with RMSE and similar error metrics)\n",
    "\n",
    "- [Serial correlation (Ljung-Box)](http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_serial_correlation.html#statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_serial_correlation)\n",
    "    - Null hypothesis is no serial correlation in residuals (independent of each other)\n",
    "\n",
    "- [Heteroskedasticity](http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_heteroskedasticity.html#statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_heteroskedasticity)\n",
    "    - Tests for change in variance between residuals.\n",
    "    - The null hypothesis is of no heteroskedasticity. That means different things depending on which alternative is selected:\n",
    "        - Increasing: Null hypothesis is that the variance is not increasing throughout the sample; that the sum-of-squares in the later subsample is not greater than the sum-of-squares in the earlier subsample.\n",
    "        - Decreasing: Null hypothesis is that the variance is not decreasing throughout the sample; that the sum-of-squares in the earlier subsample is not greater than the sum-of-squares in the later subsample.\n",
    "        - Two-sided (default): Null hypothesis is that the variance is not changing throughout the sample. Both that the sum-of-squares in the earlier subsample is not greater than the sum-of-squares in the later subsample and that the sum-of-squares in the later subsample is not greater than the sum-of-squares in the earlier subsample.\n",
    "\n",
    "- [Durbin Watson](https://en.wikipedia.org/wiki/Durbin–Watson_statistic)\n",
    "    - Tests autocorrelation of residuals: we want between 1-3, 2 is ideal (no serial correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_val, norm_p, skew, kurtosis = sar.test_normality('jarquebera')[0]\n",
    "lb_val, lb_p = sar.test_serial_correlation(method='ljungbox')[0]\n",
    "het_val, het_p = sar.test_heteroskedasticity('breakvar')[0]\n",
    "# we want to look at largest lag for Ljung-Box, so take largest number in series\n",
    "# there's intelligence in the method to determine how many lags back to calculate this stat\n",
    "lb_val = lb_val[-1]\n",
    "lb_p = lb_p[-1]\n",
    "durbin_watson = sm.stats.stattools.durbin_watson(sar.filter_results.standardized_forecasts_error[0, sar.loglikelihood_burn:])\n",
    "\n",
    "print('Normality: val={:.3f}, p={:.3f}'.format(norm_val, norm_p));\n",
    "print('Ljung-Box: val={:.3f}, p={:.3f}'.format(lb_val, lb_p));\n",
    "print('Heteroskedasticity: val={:.3f}, p={:.3f}'.format(het_val, het_p));\n",
    "print('Durbin-Watson: d={:.2f}'.format(durbin_watson))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Note on autofit methods\n",
    "R has an autoARIMA function (and other automagic methods) that gridsearches/optimizes our model parameters for us. Over time, more of these goodies are porting to Python (e.g. statsmodels.tsa.x13 import x13_arima_select_order). While there's nothing wrong with utilizing these resources, the _human makes the final determination!_ Don't become over-reliant on these methods, especially early on when you are grasping the underlying mechanics and theory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# autoselect for annual, limited to only searching AR and MA parameters\n",
    "\n",
    "autores = sm.tsa.arma_order_select_ic(annual_temp, ic=['aic', 'bic'], trend='c', max_ar=3, max_ma=3, fit_kw=dict(method='css-mle'))\n",
    "\n",
    "print('AIC', autores.aic_min_order) # will use this as inputs for annual\n",
    "print('BIC', autores.bic_min_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using itertools to gridsearch solutions\n",
    "import itertools\n",
    "\n",
    "#set parameter range; UPDATE THESE!\n",
    "p = q = range(0, 3)\n",
    "d = range(0, 2)\n",
    "season = 12\n",
    "\n",
    "# list of all parameter combos\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "# same for seasonal variant\n",
    "seasonal_pdq = [(x[0], x[1], x[2], season) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "print('SARIMAX: {} , {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} , {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} , {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} , {}'.format(pdq[2], seasonal_pdq[4]))\n",
    "\n",
    "# https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-forecasting-with-arima-in-python-3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# find optimal ARIMA for annual\n",
    "warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(annual_temp,\n",
    "                                            order=param,\n",
    "                                            seasonal_order=param_seasonal,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "\n",
    "            results = mod.fit()\n",
    "\n",
    "            print('ARIMA{},{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ARIMA with Pyflux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# helper plot\n",
    "plots(monthly_temp.temp, lags=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build and summarize model\n",
    "model = pf.ARIMA(data=annual_temp, ar=1, ma=1, integ=0, target='temp')\n",
    "x = model.fit(\"MLE\")\n",
    "x.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot z-scores of feature coefficients\n",
    "model.plot_z(indices=range(1,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot model against raw data\n",
    "model.plot_fit(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.plot_predict_is(50,figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot prediction\n",
    "model.plot_predict(h=20,past_values=20,figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# predict future values\n",
    "model.predict(h=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Predicting Sunspots with Pyflux and ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read and plot data\n",
    "data = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/datasets/sunspot.year.csv') #\n",
    "data.index = data['time'].values\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(data.index,data['sunspot.year'])\n",
    "plt.ylabel('Sunspots')\n",
    "plt.title('Yearly Sunspot Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fit and summarize model\n",
    "model = pf.ARIMA(data=data,ar=4,ma=4,integ=0,target='sunspot.year')\n",
    "x = model.fit(\"MLE\")\n",
    "x.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plot z-scores of feature coefficients\n",
    "model.plot_z(indices=range(1,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.plot_fit(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.plot_predict_is(50,figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.plot_predict(h=20,past_values=20,figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.predict(h=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notes:\n",
    "\n",
    "be careful about autoarima models in R, consider your metric/criteria.\n",
    "grab gridsearch method and tweak\n",
    "BIC vs AIC?\n",
    "\n",
    "Visual vs formal testing\n",
    "Logging and differencing for var and trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## CO2 Levels\n",
    "Statsmodels has some 'built-in' time series datasets to play with, including one that tracks atmospheric CO2 from continuous air samples at Mauna Loa Observatory in Hawaii. This data includes CO2 samples from MAR 1958 to DEC 2001.\n",
    "\n",
    "[Credits](https://www.digitalocean.com/community/users/tvincent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data = sm.datasets.co2.load_pandas()\n",
    "co2 = data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# resample to monthly and check missing values\n",
    "co2 = co2['co2'].resample('MS').mean()\n",
    "\n",
    "co2 = co2.fillna(co2.bfill())\n",
    "co2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decomposition = sm.tsa.seasonal_decompose(co2, model='additive')\n",
    "fig = decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# optimize our SARIMAX model using itertools\n",
    "#\n",
    "warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(co2,\n",
    "                                            order=param,\n",
    "                                            seasonal_order=param_seasonal,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "\n",
    "            results = mod.fit()\n",
    "\n",
    "            print('ARIMA{},{} - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": false,
   "nav_menu": {
    "height": "259px",
    "width": "395px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": "3",
   "toc_cell": false,
   "toc_position": {
    "height": "22px",
    "left": "1105px",
    "right": "20px",
    "top": "-1px",
    "width": "22px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
